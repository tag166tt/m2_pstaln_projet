{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def extract_full_sentences(file_path,test_size = None,shuffle = None):\n",
    "    random_state = 42\n",
    "    X = []\n",
    "    y = []\n",
    "    temp_X = []\n",
    "    temp_y = []\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as fin :\n",
    "        for l in fin :\n",
    "            l = l.rstrip().split(\"\\t\")\n",
    "            if l != [\"\"]:\n",
    "                temp_X.append(l[1].lower())\n",
    "                temp_y.append(l[7])\n",
    "            else :\n",
    "                X.append(temp_X)\n",
    "                temp_X = []\n",
    "                y.append(temp_y)\n",
    "                temp_y = []\n",
    "    if test_size is not None:\n",
    "        X, X_val, y, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=shuffle)\n",
    "        return X, X_val, y, y_val\n",
    "\n",
    "    else :\n",
    "        return X,y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SstDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings,labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://github.com/soutsios/pos-tagger-bert/blob/master/pos_tagger_bert.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'night', 'that', 'jam', 'master', \"jay's\", 'hand', 'slipped', '!', 'analysis', 'by', 'cosmo', 'baker', ':', 'url']\n",
      "['', 'n.time', '', 'n.body', '', '', '', 'v.motion', '', 'n.act', '', 'n.person', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = extract_full_sentences('../dimsum-data-1.5/dimsum16.train',0.15,True)\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TAGS:  42\n"
     ]
    }
   ],
   "source": [
    "tags = set([item for sublist in y_train for item in sublist])\n",
    "print('TOTAL TAGS: ', len(tags))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [],
   "source": [
    "tag2int = {}\n",
    "int2tag = {}\n",
    "\n",
    "for i, tag in enumerate(sorted(tags)):\n",
    "    tag2int[tag] = i\n",
    "    int2tag[i] = tag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [],
   "source": [
    "def map_tagging(y):\n",
    "    temp_y = []\n",
    "    for sentence in y :\n",
    "        temp_y.append([tag2int[tag] for tag in sentence])\n",
    "    return temp_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [],
   "source": [
    "y_train = map_tagging(y_train)\n",
    "y_val = map_tagging(y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 70\n",
    "EPOCHS = 30"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\eliea/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\eliea/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\eliea/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\eliea/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\eliea/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(X,y):\n",
    "    tokenized_inputs = tokenizer(X, padding=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(y):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    return tokenized_inputs, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [],
   "source": [
    "tokenized_train, padded_label_train = tokenize_and_align_labels(X_train,y_train)\n",
    "tokenized_val, padded_label_val = tokenize_and_align_labels(X_val,y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [],
   "source": [
    "train_dataset = SstDataset(tokenized_train,padded_label_train)\n",
    "val_dataset = SstDataset(tokenized_val,padded_label_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\eliea/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\eliea/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(tags))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 4084\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 768\n",
      "C:\\Users\\eliea\\anaconda3\\envs\\dataoutai\\lib\\site-packages\\transformers\\data\\data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "C:\\Users\\eliea\\anaconda3\\envs\\dataoutai\\lib\\site-packages\\transformers\\data\\data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/768 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 721\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "C:\\Users\\eliea\\anaconda3\\envs\\dataoutai\\lib\\site-packages\\transformers\\data\\data_collator.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
      "C:\\Users\\eliea\\anaconda3\\envs\\dataoutai\\lib\\site-packages\\transformers\\data\\data_collator.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 721\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 721\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=768, training_loss=0.6807648042837778, metrics={'train_runtime': 154.1681, 'train_samples_per_second': 79.472, 'train_steps_per_second': 4.982, 'total_flos': 284716166625072.0, 'train_loss': 0.6807648042837778, 'epoch': 3.0})"
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nerpipeline = pipeline('ner', model=model, tokenizer=tokenizer, device = 0)\n",
    "text = \"flutes sound really bad with cheap mics\"\n",
    "pred = nerpipeline(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flutes : n.artifact\n",
      "sound : v.cognition\n",
      "really\n",
      "bad\n",
      "with\n",
      "cheap\n",
      "mic : n.artifact\n",
      "##s : n.artifact\n"
     ]
    }
   ],
   "source": [
    "for token in pred :\n",
    "    word = token['word']\n",
    "    pred_sst = int2tag[int(token['entity'].split(\"_\")[1])]\n",
    "    print(f\"{word}\", end =\"\")\n",
    "    if pred_sst != \"\":\n",
    "        print(f\" : {pred_sst}\")\n",
    "    else :\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}